# -*- coding: utf-8 -*-
"""sentiment analysis on imdb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQS5yt9nOiBk9I1y-s2LYL2Z9FQDrX8l
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('darkgrid')

from google.colab import drive
drive.mount('/content/drive')

"""## Dataset Description"""

df_review = pd.read_excel("/content/drive/MyDrive/imdb dataset.xlsx")
df_review

df_review.info()

df_review.describe()

"""# Data Visualization
*Data visualization is the representation of data through use of common graphics, such as charts, plots, infographics, and even animations. These visual displays of information communicate complex data relationships and data-driven insights in a way that is easy to understand.*
"""

a , ax = plt.subplots(1,2,figsize=(22,9))
df_review['Sentiments'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.2f%%',ax=ax[0],shadow=True, startangle=300, colors = ["skyblue", "pink"])
ax[0].set_title('Distribution of Positive / Negative Emotions')
ax[0].set_ylabel('')
sns.countplot(data = df_review ,x='Sentiments',ax=ax[1], palette=["skyblue", "pink"])
ax[1].set_title('Distribution of Positive / Negative Emotions')
plt.show()

print(df_review.shape)

"""**ANALYSING AND VISUALIZING**

**Top 10 movies per user rating**
"""

top_per_ur = df_review[['Movie Name', 'Movie Rating']].sort_values('Movie Rating', ascending = False).head(10)

fig,ax1 = plt.subplots(1,1, figsize = (16,8))


ax1.set_title('Top 10 movies per User Rating')
ax1.set_xlim(9, 10)
sns.barplot(x = 'Movie Rating', y = 'Movie Name', data = top_per_ur, ax = ax1, palette = 'crest');

plt.tight_layout(pad= 2)

"""**WORST 10 MOVIES PER USER RATINGS**"""

worst_per_ur = df_review[['Movie Name', 'Movie Rating']].sort_values('Movie Rating', ascending = True).head(10)

fig,ax2 = plt.subplots(1,1, figsize = (16,8))


ax2.set_title('Top 10 worst movies per User Rating')
ax2.set_xlim(1,5)
sns.barplot(x = 'Movie Rating', y = 'Movie Name', data = worst_per_ur, ax = ax2, palette = 'dark:salmon');

plt.tight_layout(pad= 2)

"""**From which year are the most movies in IMDb's top 250**"""

best_years = df_review.groupby('Year of Release')['Movie Name'].count().sort_values(ascending = False).head(10)

plt.figure(figsize=(20,10))
sns.barplot(x = best_years.index , y=best_years, order = best_years.index, palette = 'Spectral' );

plt.title("Years with most movies in IMDB's top 250 list")
plt.ylabel('Number of movies');

"""**Most movies by genre in IMDB's top 250**"""

most_by_genre =  df_review.groupby('Genre')['Movie Name'].count().sort_values(ascending = False).head(10)

plt.figure(figsize=(20,10))
sns.barplot(x = most_by_genre.index, y = most_by_genre)
plt.title('Most movies by genre')
plt.ylabel('Number of movies')
plt.xticks(rotation = 70)
plt.ylim(4,22);

sns.histplot(data=df_review, x="Movie Rating", kde=True)

import plotly.express as px
ax = px.histogram(df_review,x="Watch Time",marginal="box",title="Total Time")
ax.update_layout(bargap=0.2)

"""**WORDCLOUD**

*Word clouds are popular for visualizing qualitative data because theyâ€™re simple to use and provide quick insights at a glance.
it's important to remember that while word clouds are useful for visualizing common words in a text or data set, they're usually only useful as a high-level overview of themes. They're similar to bar blots but are often more visually appealing (albeit at times harder to interpret). Word clouds can be particularly helpful when you want to:*

    **Quickly identify the most important themes or topics in a large body of text
    **Understand the overall sentiment or tone of a piece of writing
    **Explore patterns or trends in data that contain textual information
    **Communicate the key ideas or concepts in a visually engaging way
*However, it's important to keep in mind that word clouds don't provide any context or deeper understanding of the words and phrases being used. Therefore, they should be used in conjunction with other methods for analyzing and interpreting text data.*
"""

from sklearn.preprocessing import LabelEncoder
sentences=df_review['Reviews']
le=LabelEncoder()
df_review['Sentiments']= le.fit_transform(df_review['Sentiments'])

"""**GENERATING POSITIVE WORDS**"""

from wordcloud import WordCloud,STOPWORDS

stopwords = set(STOPWORDS)

pos=' '.join(map(str,sentences[df_review['Sentiments']==1]))
neg=' '.join(map(str,sentences[df_review['Sentiments']==0]))

wordcloud1 = WordCloud(width = 800, height = 800,
                background_color ='black',
                stopwords = stopwords,
                min_font_size = 10).generate(pos)

plt.figure(figsize=(8,8))
plt.imshow(wordcloud1)
plt.title('Positive Sentiment')
plt.axis('off')

"""**GENERATING NEGATIVE WORDS**"""

plt.figure(figsize=(8,8))
wordcloud2 = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(neg)

plt.imshow(wordcloud2)
plt.title('Negative Sentiment')
plt.axis('off')

plt.show()

"""**Data cleaning**"""

df_review.isna().any()

df_review.isna().sum()

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from tqdm.auto import tqdm
import time

# Clean the data
def clean_text(text):
    # Remove HTML tags
    text = re.sub('<.*?>', '', str(text))
    # Remove non-alphabetic characters and convert to lowercase
    text = re.sub('[^a-zA-Z]', ' ', str(text).lower())
    # Tokenize the text
    words = nltk.word_tokenize(text)
    # Remove stopwords
    words = [w for w in words if w not in stopwords.words('english')]
    # Stem the words
    stemmer = PorterStemmer()
    words = [stemmer.stem(w) for w in words]
    # Join the words back into a string
    text = ' '.join(words)
    return text

import nltk
nltk.download('punkt')

import nltk
nltk.download('stopwords')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# tqdm.pandas()
# 
# df_review['cleaned_text'] = df_review['Reviews'].progress_apply(clean_text)

"""Train and Test Sets"""

df_review.Sentiments.value_counts()

reviews=df_review['Reviews']
sentiment=df_review['Sentiments']

print('Classes::\n',np.unique(sentiment))

train_x=reviews[:375]
train_y=sentiment[:357]
test_x=reviews[375:]
test_y=sentiment[375:]

#Shape of train & test dataset
print('Shape of train dataset::',train_x.shape, train_y.shape)
print('Shape of test dataset::',test_x.shape, test_y.shape)

from sklearn.model_selection import train_test_split

train,test = train_test_split(df_review,test_size =0.33,random_state=42)

train_x, train_y = (train['Reviews'].values.astype('U')), (train['Sentiments'].values.astype('U'))
test_x, test_y = (test['Reviews'].values.astype('U')), (test['Sentiments'].values.astype('U'))

train_y

"""**Feature Extraction**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

train_x_vector = tfidf.fit_transform(train_x)
# also fit the test_x_vector
test_x_vector = tfidf.transform(test_x)

"""**SVC ALGORITHM**"""

from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(train_x_vector, train_y)

print(svc.predict(tfidf.transform(['A bad movie'])))
print(svc.predict(tfidf.transform(['An excellent movie'])))
print(svc.predict(tfidf.transform(['I did not like this movie at all I gave this movie away'])))

print("Accuracy of training data is:",svc.score(train_x_vector, train_y))
print("Accuracy of testing data is:",svc.score(test_x_vector, test_y))

"""**CONFUSION MATRIX FOR SVC**"""

pred1 = svc.predict(test_x_vector)
pred1[:5]

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(test_y, pred1, target_names = ['Bad Reviews','Good Reviews']))

cm1 = confusion_matrix(test_y,pred1)
cm1

plt.figure(figsize = (5,5))
sns.heatmap(cm1,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""**DECISION TREE ALGORITHM**"""

from sklearn.tree import DecisionTreeClassifier

dec_tree = DecisionTreeClassifier()
dec_tree.fit(train_x_vector, train_y)
print("Accuracy of training data is:",dec_tree.score(train_x_vector, train_y))
print("Accuracy of testing data is:",dec_tree.score(test_x_vector, test_y))

"""**CONFUSION MATRIX FOR DECISION TREE**"""

pred2 = dec_tree.predict(test_x_vector)
pred2[:5]

print(classification_report(test_y, pred2, target_names = ['Bad Reviews','Good Reviews']))

cm2 = confusion_matrix(test_y,pred2)
cm2

plt.figure(figsize = (5,5))
sns.heatmap(cm2,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""**GAUSSIAN NAIVE BAYES ALGORITHM**"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_x_vector.toarray(), train_y)
print("Accuracy of training data is:",gnb.score(train_x_vector.toarray(), train_y))
print("Accuracy of testing data is:",gnb.score(test_x_vector.toarray(), test_y))

"""**CONFUSION MATRIX FOR GAUSSIAN NAIVE BAYES**"""

pred3 = gnb.predict(test_x_vector.toarray())
pred3[:5]

print(classification_report(test_y, pred3, target_names = ['Bad Reviews','Good Reviews']))

cm3 = confusion_matrix(test_y,pred3)
cm3

plt.figure(figsize = (5,5))
sns.heatmap(cm3,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""**LOGISTIC REGRESSION ALGORITHM**"""

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(train_x_vector,train_y)
print("Accuracy of training data is:",log_reg.score(train_x_vector,train_y))
print("Accuracy of testing data is:",log_reg.score(test_x_vector, test_y))

"""**CONFUSION MATRIX FOR LOGISTIC REGRESSION**"""

pred4 = log_reg.predict(test_x_vector)
pred4[:5]

print(classification_report(test_y, pred4, target_names = ['Bad Reviews','Good Reviews']))

cm4 = confusion_matrix(test_y,pred4)
cm4

plt.figure(figsize = (5,5))
sns.heatmap(cm4,cmap= "Blues", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])
plt.xlabel("Predicted")
plt.ylabel("Actual")